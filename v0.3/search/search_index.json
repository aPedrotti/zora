{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 What is Zora? \u00b6 Zora is a multi-cluster scan that helps you to identify potential issues and vulnerabilities in your Kubernetes clusters in a centralized way, ensuring that the recommended best practices are in place. Throughout this documentation, we will use the following notation: Management Cluster to refer to the only Kubernetes cluster where Zora is installed; Target Cluster to refer to all clusters you will connect to Zora to be scanned. Follow these steps to get started with Zora: Install Zora in a Management Cluster Prepare the target cluster by creating a service account and generating a kubeconfig Connect the target cluster to Zora Configure a scan for the target cluster After a successful scan checkout the potential reported issues All the information about these steps are detailed throughout this documentation. Architecture \u00b6 Zora origins \u00b6 In the early days of the cloud native era, Borg dominated the container-oriented cluster management scene. The origin of the name Borg refers to the cybernetic life form existing in the Star Trek series, that worked as a collective of individuals with a single mind and the same purpose, as well as a \" cluster \". As good nerds as we are and wishing to honor our Kubernetes' predecessor (Borg) we named our project Zora . In Star Trek, Zora is the Artificial Intelligence that controls the ship U.S.S Discovery. After being merged with a collective of other intelligences, Zora became sentient and became a member of the team, bringing insights and making the ship more efficient. Like Star Trek's Zora, our goal is to help manage your K8s environment by periodically scanning all of your clusters, looking for potential issues or vulnerabilities with deployed features and configurations, and helping you ensure compliance with the best practices.","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#what-is-zora","text":"Zora is a multi-cluster scan that helps you to identify potential issues and vulnerabilities in your Kubernetes clusters in a centralized way, ensuring that the recommended best practices are in place. Throughout this documentation, we will use the following notation: Management Cluster to refer to the only Kubernetes cluster where Zora is installed; Target Cluster to refer to all clusters you will connect to Zora to be scanned. Follow these steps to get started with Zora: Install Zora in a Management Cluster Prepare the target cluster by creating a service account and generating a kubeconfig Connect the target cluster to Zora Configure a scan for the target cluster After a successful scan checkout the potential reported issues All the information about these steps are detailed throughout this documentation.","title":"What is Zora?"},{"location":"#architecture","text":"","title":"Architecture"},{"location":"#zora-origins","text":"In the early days of the cloud native era, Borg dominated the container-oriented cluster management scene. The origin of the name Borg refers to the cybernetic life form existing in the Star Trek series, that worked as a collective of individuals with a single mind and the same purpose, as well as a \" cluster \". As good nerds as we are and wishing to honor our Kubernetes' predecessor (Borg) we named our project Zora . In Star Trek, Zora is the Artificial Intelligence that controls the ship U.S.S Discovery. After being merged with a collective of other intelligences, Zora became sentient and became a member of the team, bringing insights and making the ship more efficient. Like Star Trek's Zora, our goal is to help manage your K8s environment by periodically scanning all of your clusters, looking for potential issues or vulnerabilities with deployed features and configurations, and helping you ensure compliance with the best practices.","title":"Zora origins"},{"location":"cluster-scan/","text":"Configure a cluster scan \u00b6 Since your clusters are connected the next and last step is configure a scan for them by creating a ClusterScan in the same namespace as Cluster resource. The ClusterScan will be responsible for reporting issues and vulnerabilities of your clusters. Failure to perform this step implies that the scan will not be performed, and therefore the health of your cluster will be unknown. Create a ClusterScan \u00b6 The ClusterScan scans the Cluster referenced in clusterRef.name field periodically on a given schedule, written in Cron format. Here is a sample configuration that scan mycluster once an hour. You can modify putting your desired periodicity. cat << EOF | kubectl apply -f - apiVersion : zora.undistro.io/v1alpha1 kind : ClusterScan metadata : name : mycluster spec : clusterRef : name : mycluster schedule : \"0 */1 * * *\" EOF Cron schedule syntax \u00b6 Cron expression has five fields separated by a space, and each field represents a time unit. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday; \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) \u2502 \u2502 \u2502 \u2502 \u2502 OR sun, mon, tue, wed, thu, fri, sat \u2502 \u2502 \u2502 \u2502 \u2502 * * * * * Operator Descriptor Example * Any value 15 * * * * runs at every minute 15 of every hour of every day. , Value list separator 2,10 4,5 * * * runs at minute 2 and 10 of the 4th and 5th hour of every day. - Range of values 30 4-6 * * * runs at minute 30 of the 4th, 5th, and 6th hour. / Step values 20/15 * * * * runs every 15 minutes starting from minute 20 through 59 (minutes 20, 35, and 50). Now Zora is ready to help you to identify potential issues and vulnerabilities in your kubernetes clusters. You can check the scans status and the reported issues by the following steps: List cluster scans \u00b6 Listing the ClusterScans , the information of the last scans are available: kubectl get clusterscan -o wide NAME CLUSTER SCHEDULE SUSPEND PLUGINS LAST STATUS LAST SCHEDULE LAST SUCCESSFUL ISSUES READY AGE NEXT SCHEDULE mycluster mycluster 0 */1 * * * false popeye Complete 12m 14m 21 True 32d 2022 -06-27T23:00:00Z The LAST STATUS column represents the status (Active, Complete or Failed) of the last scan that was scheduled at the time represented by LAST SCHEDULE column. List cluster issues \u00b6 Once the cluster is successfully scanned, the reported issues are available in ClusterIssue resources: kubectl get clusterissues -l cluster = mycluster NAME CLUSTER ID MESSAGE SEVERITY CATEGORY AGE mycluster-pop-102-27557035 mycluster POP-102 No probes defined Medium pods 4m8s mycluster-pop-105-27557035 mycluster POP-105 Liveness probe uses a port#, prefer a named port Low pods 4m8s mycluster-pop-106-27557035 mycluster POP-106 No resources requests/limits defined Medium daemonsets 4m8s mycluster-pop-1100-27557035 mycluster POP-1100 No pods match service selector High services 4m8s mycluster-pop-306-27557035 mycluster POP-306 Container could be running as root user. Check SecurityContext/Image Medium pods 4m8s mycluster-pop-500-27557035 mycluster POP-500 Zero scale detected Medium deployments 4m8s It's possible filter issues by cluster, issue ID, severity and category using label selector : # issues from mycluster kubectl get clusterissues -l cluster = mycluster # clusters with issue POP-106 kubectl get clusterissues -l id = POP-106 # issues from mycluster with high severity kubectl get clusterissues -l cluster = mycluster,severity = High # only issues reported by the last scan from mycluster kubectl get clusterissues -l cluster = mycluster,scanID = fa4e63cc-5236-40f3-aa7f-599e1c83208b Why is it an issue? The field url in ClusterIssue spec represents a link for a documentation about this issue. It is displayed in the UI and you can see by kubectl with the -o=yaml flag or the command below. kubectl get clusterissues -o = custom-columns = \"NAME:.metadata.name,MESSAGE:.spec.message,URL:.spec.url\" NAME MESSAGE URL mycluster-pop-102-27557035 No probes defined https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ mycluster-pop-105-27557035 Liveness probe uses a port#, prefer a named port <none> mycluster-pop-106-27557035 No resources requests/limits defined https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ mycluster-pop-1100-27557035 No pods match service selector https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service mycluster-pop-306-27557035 Container could be running as root user. Check SecurityContext/Image https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted mycluster-pop-500-27557035 Zero scale detected https://kubernetes.io/docs/concepts/workloads/ These docs should help you understand why it's an issue and how to fix it. All URLs are available here and you can contribute to Zora adding new links. See our contribution guidelines .","title":"Configure a cluster scan"},{"location":"cluster-scan/#configure-a-cluster-scan","text":"Since your clusters are connected the next and last step is configure a scan for them by creating a ClusterScan in the same namespace as Cluster resource. The ClusterScan will be responsible for reporting issues and vulnerabilities of your clusters. Failure to perform this step implies that the scan will not be performed, and therefore the health of your cluster will be unknown.","title":"Configure a cluster scan"},{"location":"cluster-scan/#create-a-clusterscan","text":"The ClusterScan scans the Cluster referenced in clusterRef.name field periodically on a given schedule, written in Cron format. Here is a sample configuration that scan mycluster once an hour. You can modify putting your desired periodicity. cat << EOF | kubectl apply -f - apiVersion : zora.undistro.io/v1alpha1 kind : ClusterScan metadata : name : mycluster spec : clusterRef : name : mycluster schedule : \"0 */1 * * *\" EOF","title":"Create a ClusterScan"},{"location":"cluster-scan/#cron-schedule-syntax","text":"Cron expression has five fields separated by a space, and each field represents a time unit. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday; \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) \u2502 \u2502 \u2502 \u2502 \u2502 OR sun, mon, tue, wed, thu, fri, sat \u2502 \u2502 \u2502 \u2502 \u2502 * * * * * Operator Descriptor Example * Any value 15 * * * * runs at every minute 15 of every hour of every day. , Value list separator 2,10 4,5 * * * runs at minute 2 and 10 of the 4th and 5th hour of every day. - Range of values 30 4-6 * * * runs at minute 30 of the 4th, 5th, and 6th hour. / Step values 20/15 * * * * runs every 15 minutes starting from minute 20 through 59 (minutes 20, 35, and 50). Now Zora is ready to help you to identify potential issues and vulnerabilities in your kubernetes clusters. You can check the scans status and the reported issues by the following steps:","title":"Cron schedule syntax"},{"location":"cluster-scan/#list-cluster-scans","text":"Listing the ClusterScans , the information of the last scans are available: kubectl get clusterscan -o wide NAME CLUSTER SCHEDULE SUSPEND PLUGINS LAST STATUS LAST SCHEDULE LAST SUCCESSFUL ISSUES READY AGE NEXT SCHEDULE mycluster mycluster 0 */1 * * * false popeye Complete 12m 14m 21 True 32d 2022 -06-27T23:00:00Z The LAST STATUS column represents the status (Active, Complete or Failed) of the last scan that was scheduled at the time represented by LAST SCHEDULE column.","title":"List cluster scans"},{"location":"cluster-scan/#list-cluster-issues","text":"Once the cluster is successfully scanned, the reported issues are available in ClusterIssue resources: kubectl get clusterissues -l cluster = mycluster NAME CLUSTER ID MESSAGE SEVERITY CATEGORY AGE mycluster-pop-102-27557035 mycluster POP-102 No probes defined Medium pods 4m8s mycluster-pop-105-27557035 mycluster POP-105 Liveness probe uses a port#, prefer a named port Low pods 4m8s mycluster-pop-106-27557035 mycluster POP-106 No resources requests/limits defined Medium daemonsets 4m8s mycluster-pop-1100-27557035 mycluster POP-1100 No pods match service selector High services 4m8s mycluster-pop-306-27557035 mycluster POP-306 Container could be running as root user. Check SecurityContext/Image Medium pods 4m8s mycluster-pop-500-27557035 mycluster POP-500 Zero scale detected Medium deployments 4m8s It's possible filter issues by cluster, issue ID, severity and category using label selector : # issues from mycluster kubectl get clusterissues -l cluster = mycluster # clusters with issue POP-106 kubectl get clusterissues -l id = POP-106 # issues from mycluster with high severity kubectl get clusterissues -l cluster = mycluster,severity = High # only issues reported by the last scan from mycluster kubectl get clusterissues -l cluster = mycluster,scanID = fa4e63cc-5236-40f3-aa7f-599e1c83208b Why is it an issue? The field url in ClusterIssue spec represents a link for a documentation about this issue. It is displayed in the UI and you can see by kubectl with the -o=yaml flag or the command below. kubectl get clusterissues -o = custom-columns = \"NAME:.metadata.name,MESSAGE:.spec.message,URL:.spec.url\" NAME MESSAGE URL mycluster-pop-102-27557035 No probes defined https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ mycluster-pop-105-27557035 Liveness probe uses a port#, prefer a named port <none> mycluster-pop-106-27557035 No resources requests/limits defined https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ mycluster-pop-1100-27557035 No pods match service selector https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service mycluster-pop-306-27557035 Container could be running as root user. Check SecurityContext/Image https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted mycluster-pop-500-27557035 Zero scale detected https://kubernetes.io/docs/concepts/workloads/ These docs should help you understand why it's an issue and how to fix it. All URLs are available here and you can contribute to Zora adding new links. See our contribution guidelines .","title":"List cluster issues"},{"location":"connect-cluster/","text":"Connect the target cluster to Zora \u00b6 After preparing your target clusters , you need to connect them directly to Zora by following the instructions below. Prerequisites \u00b6 A kubeconfig file with an authentication token of the target cluster. Follow these instructions to generate it. The api-server of the target cluster must be reachable by the management cluster . Without the prerequisites Zora will not be able to connect to the target cluster and will set a failure status. Metrics Server If the target cluster hasn't Metrics Server deployed, information about the usage of memory and CPU won't be collected and issues about potential resources over/under allocations won't be reported. For more information about Metrics Server, visit the official documentation . 1. Access the management cluster \u00b6 First, make sure you are in the context of the management cluster . You can do this by the following commands: Display list of contexts: kubectl config get-contexts Display the current-context: kubectl config current-context Set the default context to my-management-cluster : kubectl config use-context my-management-cluster 2. Create a Cluster resource \u00b6 First, create a Secret with the content of the kubeconfig file: kubectl create secret generic mycluster-kubeconfig \\ -n zora-system \\ --from-file = value = zora-view-kubeconfig.yml Now, you are able to create a Cluster resource referencing the kubeconfig Secret in the same namespace: cat << EOF | kubectl apply -f - apiVersion : zora.undistro.io/v1alpha1 kind : Cluster metadata : name : mycluster namespace : zora-system labels : zora.undistro.io/environment : prod spec : kubeconfigRef : name : mycluster-kubeconfig EOF If you've made it this far, congratulations, your clusters are connected. Now you can list them and see the discovered data through kubectl : List clusters \u00b6 kubectl get clusters -o wide NAME VERSION MEM AVAILABLE MEM USAGE ( % ) CPU AVAILABLE CPU USAGE ( % ) NODES READY AGE PROVIDER REGION mycluster v1.21.5-eks-bc4871b 10033Mi 3226Mi ( 32 % ) 5790m 647m ( 11 % ) 3 True 40d aws us-east-1 Tip Get clusters from all namespaces using --all-namespaces flag Get clusters with additional information using -o=wide flag Get the documentation for clusters manifests using kubectl explain clusters Get cluster from prod environment using kubectl get clusters -l zora.undistro.io/environment=prod The cluster list output has the following columns: NAME : Cluster name VERSION : Kubernetes version MEM AVAILABLE : Quantity of memory available (requires Metrics Server) MEM USAGE (%) : Usage of memory in quantity and percentage (requires Metrics Server) CPU AVAILABLE : Quantity of CPU available (requires Metrics Server) CPU USAGE (%) : Usage of CPU in quantity and percentage (requires Metrics Server) NODES : Total of nodes READY : Indicates whether the cluster is connected AGE : Age of the kube-system namespace in cluster PROVIDER : Cluster provider (with -o=wide flag) REGION : Cluster region ( multi-region if nodes have different topology.kubernetes.io/region label) (with -o=wide flag) Provider The value in PROVIDER column is obtained by matching the Node's labels (e.g., a Node with label key prefix eks.amazonaws.com/ means that the provider of this cluster is aws ). For now, Zora recognizes only the providers in this list . But you can connect clusters of any provider. If the provider isn't in this list, the column will not be filled and Zora will continue to work normally. Fell free to contribute to the project and add new labels prefixes for providers. See our contribution guidelines . Info The quantity of available and in use resources, is a sum of all Nodes. Only one provider is displayed in PROVIDER column. Different information can be displayed for multi-cloud clusters. Show detailed description of a cluster, including events , running kubectl describe cluster mycluster .","title":"Connect the target cluster to Zora"},{"location":"connect-cluster/#connect-the-target-cluster-to-zora","text":"After preparing your target clusters , you need to connect them directly to Zora by following the instructions below.","title":"Connect the target cluster to Zora"},{"location":"connect-cluster/#prerequisites","text":"A kubeconfig file with an authentication token of the target cluster. Follow these instructions to generate it. The api-server of the target cluster must be reachable by the management cluster . Without the prerequisites Zora will not be able to connect to the target cluster and will set a failure status. Metrics Server If the target cluster hasn't Metrics Server deployed, information about the usage of memory and CPU won't be collected and issues about potential resources over/under allocations won't be reported. For more information about Metrics Server, visit the official documentation .","title":"Prerequisites"},{"location":"connect-cluster/#1-access-the-management-cluster","text":"First, make sure you are in the context of the management cluster . You can do this by the following commands: Display list of contexts: kubectl config get-contexts Display the current-context: kubectl config current-context Set the default context to my-management-cluster : kubectl config use-context my-management-cluster","title":"1. Access the management cluster"},{"location":"connect-cluster/#2-create-a-cluster-resource","text":"First, create a Secret with the content of the kubeconfig file: kubectl create secret generic mycluster-kubeconfig \\ -n zora-system \\ --from-file = value = zora-view-kubeconfig.yml Now, you are able to create a Cluster resource referencing the kubeconfig Secret in the same namespace: cat << EOF | kubectl apply -f - apiVersion : zora.undistro.io/v1alpha1 kind : Cluster metadata : name : mycluster namespace : zora-system labels : zora.undistro.io/environment : prod spec : kubeconfigRef : name : mycluster-kubeconfig EOF If you've made it this far, congratulations, your clusters are connected. Now you can list them and see the discovered data through kubectl :","title":"2. Create a Cluster resource"},{"location":"connect-cluster/#list-clusters","text":"kubectl get clusters -o wide NAME VERSION MEM AVAILABLE MEM USAGE ( % ) CPU AVAILABLE CPU USAGE ( % ) NODES READY AGE PROVIDER REGION mycluster v1.21.5-eks-bc4871b 10033Mi 3226Mi ( 32 % ) 5790m 647m ( 11 % ) 3 True 40d aws us-east-1 Tip Get clusters from all namespaces using --all-namespaces flag Get clusters with additional information using -o=wide flag Get the documentation for clusters manifests using kubectl explain clusters Get cluster from prod environment using kubectl get clusters -l zora.undistro.io/environment=prod The cluster list output has the following columns: NAME : Cluster name VERSION : Kubernetes version MEM AVAILABLE : Quantity of memory available (requires Metrics Server) MEM USAGE (%) : Usage of memory in quantity and percentage (requires Metrics Server) CPU AVAILABLE : Quantity of CPU available (requires Metrics Server) CPU USAGE (%) : Usage of CPU in quantity and percentage (requires Metrics Server) NODES : Total of nodes READY : Indicates whether the cluster is connected AGE : Age of the kube-system namespace in cluster PROVIDER : Cluster provider (with -o=wide flag) REGION : Cluster region ( multi-region if nodes have different topology.kubernetes.io/region label) (with -o=wide flag) Provider The value in PROVIDER column is obtained by matching the Node's labels (e.g., a Node with label key prefix eks.amazonaws.com/ means that the provider of this cluster is aws ). For now, Zora recognizes only the providers in this list . But you can connect clusters of any provider. If the provider isn't in this list, the column will not be filled and Zora will continue to work normally. Fell free to contribute to the project and add new labels prefixes for providers. See our contribution guidelines . Info The quantity of available and in use resources, is a sum of all Nodes. Only one provider is displayed in PROVIDER column. Different information can be displayed for multi-cloud clusters. Show detailed description of a cluster, including events , running kubectl describe cluster mycluster .","title":"List clusters"},{"location":"glossary/","text":"Glossary \u00b6 Management Cluster \u00b6 The only Kubernetes cluster where Zora is installed. Target Cluster \u00b6 The Kubernetes cluster that you connect to Zora to be scanned.","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#management-cluster","text":"The only Kubernetes cluster where Zora is installed.","title":"Management Cluster"},{"location":"glossary/#target-cluster","text":"The Kubernetes cluster that you connect to Zora to be scanned.","title":"Target Cluster"},{"location":"install/","text":"Install \u00b6 Zora requires an existing Kubernetes cluster accessible via kubectl . After the installation process this cluster will be your management cluster with the Zora components installed. So it is recommended to keep it separated from any application workload. Setup Requirements \u00b6 Zora's management cluster requires these programs in order to be installed and configured: Kubernetes >= 1.21.0 Helm >= 3.4.0 Kubectl Awk Cat POSIX shell Install with Helm \u00b6 To install Zora using Helm follow these commands: helm repo add undistro https://charts.undistro.io --force-update helm repo update undistro helm upgrade --install zora undistro/zora \\ -n zora-system \\ --create-namespace --wait Info The Helm chart repository has been updated from https://registry.undistro.io/chartrepo/library to https://charts.undistro.io . The --force-update flag is needed to update the repository URL. These commands deploy Zora to the Kubernetes cluster. This section lists the parameters that can be configured during installation. Access to the UI \u00b6 The output of helm install and helm upgrade commands contains instructions to access Zora UI based on the provided parameters. You can get the instructions anytime by running: helm get notes zora -n zora-system Uninstall \u00b6 helm delete zora -n zora-system kubectl delete namespace zora-system","title":"Install"},{"location":"install/#install","text":"Zora requires an existing Kubernetes cluster accessible via kubectl . After the installation process this cluster will be your management cluster with the Zora components installed. So it is recommended to keep it separated from any application workload.","title":"Install"},{"location":"install/#setup-requirements","text":"Zora's management cluster requires these programs in order to be installed and configured: Kubernetes >= 1.21.0 Helm >= 3.4.0 Kubectl Awk Cat POSIX shell","title":"Setup Requirements"},{"location":"install/#install-with-helm","text":"To install Zora using Helm follow these commands: helm repo add undistro https://charts.undistro.io --force-update helm repo update undistro helm upgrade --install zora undistro/zora \\ -n zora-system \\ --create-namespace --wait Info The Helm chart repository has been updated from https://registry.undistro.io/chartrepo/library to https://charts.undistro.io . The --force-update flag is needed to update the repository URL. These commands deploy Zora to the Kubernetes cluster. This section lists the parameters that can be configured during installation.","title":"Install with Helm"},{"location":"install/#access-to-the-ui","text":"The output of helm install and helm upgrade commands contains instructions to access Zora UI based on the provided parameters. You can get the instructions anytime by running: helm get notes zora -n zora-system","title":"Access to the UI"},{"location":"install/#uninstall","text":"helm delete zora -n zora-system kubectl delete namespace zora-system","title":"Uninstall"},{"location":"target-cluster/","text":"Prepare the target cluster \u00b6 Follow this guide to create a service account and generate a kubeconfig file from a target cluster . These are the only steps required to be performed in the target cluster. For manual configuration, go to Manual Configuration , otherwise proceed to Setup Script . Note If your target cluster is under a server proxy for external communication, like those present on platforms like Rancher , we recommend generating a kubeconfig file through your own platform. Normally these platforms handle their own tokens instead of a service account token. Zora requires read-only access, as described here . Setup Script \u00b6 A script is available to prepare a cluster, which can be executed by any POSIX compliant shell. The target cluster context can be set by exporting the CONTEXT variable or switching via kubectl , before running the script: curl -q https://zora.undistro.io/targetcluster.sh | CONTEXT=<TARGET_CONTEXT> sh or kubectl config use-context <TARGET_CONTEXT> curl -q https://zora.undistro.io/targetcluster.sh | sh By default, the generated kubeconfig will be named as your current Kubernetes context suffixed with -kubeconfig.yaml . Before finishing, the script will show a command to connect the target cluster through the generated kubeconfig, and save a sample Cluster manifest. A complete list of customizable environment variables can be seen on the table below. Environment Variable Description SVC_ACCOUNT_NS Service Account namespace, defaults to zora-system SVC_ACCOUNT_NAME Service Account name, defaults to zora-view CLUSTER_ROLE_NAME Cluster Role name, defaults to zora-view SVC_ACCOUNT_SECRET_NS Service Account Secret namespace, defaults to the value of SVC_ACCOUNT_NS SVC_ACCOUNT_SECRET_NAME Service Account Secret name, defaults to the of value of SVC_ACCOUNT_NAME with the \"-token\" suffix KCONFIG_SECRET_NAME Name of the displayed kubeconfig Secret, defaults to the value of CLUSTER_NAME with the \"-kubeconfig\" suffix TOKEN_NAME Uses the value of SVC_ACCOUNT_SECRET_NAME or the one generated by K8s, according to the cluster version CONTEXT K8s context, using the current one as default CLUSTER_NAME Cluster name from the CONTEXT variable CLUSTER_NS Cluster namespace used on the manifest sample, defaults to the value of SVC_ACCOUNT_NS CLUSTER_CA Cluster Certificate Authority, extracted according to CONTEXT CLUSTER_SERVER Cluster server address, extracted according to CONTEXT KCONFIG_NAME Name of the generated kubeconfig, defaulting to the value of CONTEXT plus the string \"_kubeconfig.yaml\" SAMPLE_MANIFEST_NAME Name of the Cluster manifest sample, defaults to cluster_sample.yaml plus the K8s context as prefix The next instructions explain how to manually configure your target clusters. Manual Configuration \u00b6 The target cluster can be configured through the steps described in the next sections. 1. Access the target cluster \u00b6 First, make sure you are in the context of the target cluster . You can do this by the following commands: Display list of contexts: kubectl config get-contexts Display the current-context: kubectl config current-context Set the default context to my-target-cluster : kubectl config use-context my-target-cluster 2. Create the RBAC resources \u00b6 Create the service account in a separate namespace and configure view permissions. The token generated by this service account will be used in the kubeconfig file. Important You should create a separate service account in the target cluster to connect it to Zora. This is required because the kubeconfig files generated by most cloud providers, call CLI commands, such as aws or gcloud , those can\u2019t be called by Zora. kubectl create namespace zora-system kubectl -n zora-system create serviceaccount zora-view cat << EOF | kubectl apply -f - apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : zora-view rules : - apiGroups : [ \"\" ] resources : - configmaps - endpoints - limitranges - namespaces - nodes - persistentvolumes - persistentvolumeclaims - pods - replicationcontrollers - secrets - serviceaccounts - services verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"apps\" ] resources : - daemonsets - deployments - statefulsets - replicasets verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"autoscaling\" ] resources : - horizontalpodautoscalers verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"networking.k8s.io\" ] resources : - ingresses - networkpolicies verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"policy\" ] resources : - poddisruptionbudgets - podsecuritypolicies verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"rbac.authorization.k8s.io\" ] resources : - clusterroles - clusterrolebindings - roles - rolebindings verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"metrics.k8s.io\" ] resources : - pods - nodes verbs : [ \"get\" , \"list\" ] - apiGroups : [ batch ] resources : - jobs - cronjobs verbs : [ \"get\" , \"list\" ] - apiGroups : [ admissionregistration.k8s.io ] resources : - validatingwebhookconfigurations - mutatingwebhookconfigurations verbs : [ \"get\" , \"list\" ] EOF kubectl create clusterrolebinding zora-view --clusterrole=zora-view --serviceaccount=zora-system:zora-view Info Zora requires just view permissions of your target clusters. 3. Set up the environment variables \u00b6 Set up the following environment variables based on the Kubernetes version of the target cluster. You can verify the version of your cluster by running: kubectl version The Server Version is the version of Kubernetes your target cluster is running. Kubernetes prior to 1.24.0 Kubernetes 1.24.0 or later export TOKEN_NAME = $( kubectl -n zora-system get serviceaccount zora-view -o = jsonpath = '{.secrets[0].name}' ) export TOKEN_VALUE = $( kubectl -n zora-system get secret ${ TOKEN_NAME } -o = jsonpath = '{.data.token}' | base64 --decode ) export CURRENT_CONTEXT = $( kubectl config current-context ) export CURRENT_CLUSTER = $( kubectl config view --raw -o = go-template = '{{range .contexts}}{{if eq .name \"''' ${ CURRENT_CONTEXT } '''\"}}{{ index .context \"cluster\" }}{{end}}{{end}}' ) export CLUSTER_CA = $( kubectl config view --raw -o = go-template = '{{range .clusters}}{{if eq .name \"''' ${ CURRENT_CLUSTER } '''\"}}\"{{with index .cluster \"certificate-authority-data\" }}{{.}}{{end}}\"{{ end }}{{ end }}' ) export CLUSTER_SERVER = $( kubectl config view --raw -o = go-template = '{{range .clusters}}{{if eq .name \"''' ${ CURRENT_CLUSTER } '''\"}}{{ .cluster.server }}{{end}}{{ end }}' ) export TOKEN_NAME = \"zora-view-token\" cat << EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: \"$TOKEN_NAME\" namespace: \"zora-system\" annotations: kubernetes.io/service-account.name: \"zora-view\" type: kubernetes.io/service-account-token EOF export TOKEN_VALUE = $( kubectl -n zora-system get secret ${ TOKEN_NAME } -o = jsonpath = '{.data.token}' | base64 --decode ) export CURRENT_CONTEXT = $( kubectl config current-context ) export CURRENT_CLUSTER = $( kubectl config view --raw -o = go-template = '{{range .contexts}}{{if eq .name \"''' ${ CURRENT_CONTEXT } '''\"}}{{ index .context \"cluster\" }}{{end}}{{end}}' ) export CLUSTER_CA = $( kubectl config view --raw -o = go-template = '{{range .clusters}}{{if eq .name \"''' ${ CURRENT_CLUSTER } '''\"}}\"{{with index .cluster \"certificate-authority-data\" }}{{.}}{{end}}\"{{ end }}{{ end }}' ) export CLUSTER_SERVER = $( kubectl config view --raw -o = go-template = '{{range .clusters}}{{if eq .name \"''' ${ CURRENT_CLUSTER } '''\"}}{{ .cluster.server }}{{end}}{{ end }}' ) 4. Generate a kubeconfig file \u00b6 Generate a file with kubeconfig data, based on the environment variables defined before: cat << EOF > zora-view-kubeconfig.yml apiVersion : v1 kind : Config current-context : ${CURRENT_CONTEXT} contexts : - name : ${CURRENT_CONTEXT} context : cluster : ${CURRENT_CONTEXT} user : zora-view clusters : - name : ${CURRENT_CONTEXT} cluster : certificate-authority-data : ${CLUSTER_CA} server : ${CLUSTER_SERVER} users : - name : zora-view user : token : ${TOKEN_VALUE} EOF Verify the generated kubeconfig \u00b6 These steps create a file in your current working directory called zora-view-kubeconfig.yml . The contents of this file are used in the next guide to connect this target cluster into Zora. Before using this kubeconfig, you can verify that it is functional by running: kubectl --kubeconfig zora-view-kubeconfig.yml get all --all-namespaces","title":"Prepare the target cluster"},{"location":"target-cluster/#prepare-the-target-cluster","text":"Follow this guide to create a service account and generate a kubeconfig file from a target cluster . These are the only steps required to be performed in the target cluster. For manual configuration, go to Manual Configuration , otherwise proceed to Setup Script . Note If your target cluster is under a server proxy for external communication, like those present on platforms like Rancher , we recommend generating a kubeconfig file through your own platform. Normally these platforms handle their own tokens instead of a service account token. Zora requires read-only access, as described here .","title":"Prepare the target cluster"},{"location":"target-cluster/#setup-script","text":"A script is available to prepare a cluster, which can be executed by any POSIX compliant shell. The target cluster context can be set by exporting the CONTEXT variable or switching via kubectl , before running the script: curl -q https://zora.undistro.io/targetcluster.sh | CONTEXT=<TARGET_CONTEXT> sh or kubectl config use-context <TARGET_CONTEXT> curl -q https://zora.undistro.io/targetcluster.sh | sh By default, the generated kubeconfig will be named as your current Kubernetes context suffixed with -kubeconfig.yaml . Before finishing, the script will show a command to connect the target cluster through the generated kubeconfig, and save a sample Cluster manifest. A complete list of customizable environment variables can be seen on the table below. Environment Variable Description SVC_ACCOUNT_NS Service Account namespace, defaults to zora-system SVC_ACCOUNT_NAME Service Account name, defaults to zora-view CLUSTER_ROLE_NAME Cluster Role name, defaults to zora-view SVC_ACCOUNT_SECRET_NS Service Account Secret namespace, defaults to the value of SVC_ACCOUNT_NS SVC_ACCOUNT_SECRET_NAME Service Account Secret name, defaults to the of value of SVC_ACCOUNT_NAME with the \"-token\" suffix KCONFIG_SECRET_NAME Name of the displayed kubeconfig Secret, defaults to the value of CLUSTER_NAME with the \"-kubeconfig\" suffix TOKEN_NAME Uses the value of SVC_ACCOUNT_SECRET_NAME or the one generated by K8s, according to the cluster version CONTEXT K8s context, using the current one as default CLUSTER_NAME Cluster name from the CONTEXT variable CLUSTER_NS Cluster namespace used on the manifest sample, defaults to the value of SVC_ACCOUNT_NS CLUSTER_CA Cluster Certificate Authority, extracted according to CONTEXT CLUSTER_SERVER Cluster server address, extracted according to CONTEXT KCONFIG_NAME Name of the generated kubeconfig, defaulting to the value of CONTEXT plus the string \"_kubeconfig.yaml\" SAMPLE_MANIFEST_NAME Name of the Cluster manifest sample, defaults to cluster_sample.yaml plus the K8s context as prefix The next instructions explain how to manually configure your target clusters.","title":"Setup Script"},{"location":"target-cluster/#manual-configuration","text":"The target cluster can be configured through the steps described in the next sections.","title":"Manual Configuration"},{"location":"target-cluster/#1-access-the-target-cluster","text":"First, make sure you are in the context of the target cluster . You can do this by the following commands: Display list of contexts: kubectl config get-contexts Display the current-context: kubectl config current-context Set the default context to my-target-cluster : kubectl config use-context my-target-cluster","title":"1. Access the target cluster"},{"location":"target-cluster/#2-create-the-rbac-resources","text":"Create the service account in a separate namespace and configure view permissions. The token generated by this service account will be used in the kubeconfig file. Important You should create a separate service account in the target cluster to connect it to Zora. This is required because the kubeconfig files generated by most cloud providers, call CLI commands, such as aws or gcloud , those can\u2019t be called by Zora. kubectl create namespace zora-system kubectl -n zora-system create serviceaccount zora-view cat << EOF | kubectl apply -f - apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : zora-view rules : - apiGroups : [ \"\" ] resources : - configmaps - endpoints - limitranges - namespaces - nodes - persistentvolumes - persistentvolumeclaims - pods - replicationcontrollers - secrets - serviceaccounts - services verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"apps\" ] resources : - daemonsets - deployments - statefulsets - replicasets verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"autoscaling\" ] resources : - horizontalpodautoscalers verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"networking.k8s.io\" ] resources : - ingresses - networkpolicies verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"policy\" ] resources : - poddisruptionbudgets - podsecuritypolicies verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"rbac.authorization.k8s.io\" ] resources : - clusterroles - clusterrolebindings - roles - rolebindings verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"metrics.k8s.io\" ] resources : - pods - nodes verbs : [ \"get\" , \"list\" ] - apiGroups : [ batch ] resources : - jobs - cronjobs verbs : [ \"get\" , \"list\" ] - apiGroups : [ admissionregistration.k8s.io ] resources : - validatingwebhookconfigurations - mutatingwebhookconfigurations verbs : [ \"get\" , \"list\" ] EOF kubectl create clusterrolebinding zora-view --clusterrole=zora-view --serviceaccount=zora-system:zora-view Info Zora requires just view permissions of your target clusters.","title":"2. Create the RBAC resources"},{"location":"target-cluster/#3-set-up-the-environment-variables","text":"Set up the following environment variables based on the Kubernetes version of the target cluster. You can verify the version of your cluster by running: kubectl version The Server Version is the version of Kubernetes your target cluster is running. Kubernetes prior to 1.24.0 Kubernetes 1.24.0 or later export TOKEN_NAME = $( kubectl -n zora-system get serviceaccount zora-view -o = jsonpath = '{.secrets[0].name}' ) export TOKEN_VALUE = $( kubectl -n zora-system get secret ${ TOKEN_NAME } -o = jsonpath = '{.data.token}' | base64 --decode ) export CURRENT_CONTEXT = $( kubectl config current-context ) export CURRENT_CLUSTER = $( kubectl config view --raw -o = go-template = '{{range .contexts}}{{if eq .name \"''' ${ CURRENT_CONTEXT } '''\"}}{{ index .context \"cluster\" }}{{end}}{{end}}' ) export CLUSTER_CA = $( kubectl config view --raw -o = go-template = '{{range .clusters}}{{if eq .name \"''' ${ CURRENT_CLUSTER } '''\"}}\"{{with index .cluster \"certificate-authority-data\" }}{{.}}{{end}}\"{{ end }}{{ end }}' ) export CLUSTER_SERVER = $( kubectl config view --raw -o = go-template = '{{range .clusters}}{{if eq .name \"''' ${ CURRENT_CLUSTER } '''\"}}{{ .cluster.server }}{{end}}{{ end }}' ) export TOKEN_NAME = \"zora-view-token\" cat << EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: \"$TOKEN_NAME\" namespace: \"zora-system\" annotations: kubernetes.io/service-account.name: \"zora-view\" type: kubernetes.io/service-account-token EOF export TOKEN_VALUE = $( kubectl -n zora-system get secret ${ TOKEN_NAME } -o = jsonpath = '{.data.token}' | base64 --decode ) export CURRENT_CONTEXT = $( kubectl config current-context ) export CURRENT_CLUSTER = $( kubectl config view --raw -o = go-template = '{{range .contexts}}{{if eq .name \"''' ${ CURRENT_CONTEXT } '''\"}}{{ index .context \"cluster\" }}{{end}}{{end}}' ) export CLUSTER_CA = $( kubectl config view --raw -o = go-template = '{{range .clusters}}{{if eq .name \"''' ${ CURRENT_CLUSTER } '''\"}}\"{{with index .cluster \"certificate-authority-data\" }}{{.}}{{end}}\"{{ end }}{{ end }}' ) export CLUSTER_SERVER = $( kubectl config view --raw -o = go-template = '{{range .clusters}}{{if eq .name \"''' ${ CURRENT_CLUSTER } '''\"}}{{ .cluster.server }}{{end}}{{ end }}' )","title":"3. Set up the environment variables"},{"location":"target-cluster/#4-generate-a-kubeconfig-file","text":"Generate a file with kubeconfig data, based on the environment variables defined before: cat << EOF > zora-view-kubeconfig.yml apiVersion : v1 kind : Config current-context : ${CURRENT_CONTEXT} contexts : - name : ${CURRENT_CONTEXT} context : cluster : ${CURRENT_CONTEXT} user : zora-view clusters : - name : ${CURRENT_CONTEXT} cluster : certificate-authority-data : ${CLUSTER_CA} server : ${CLUSTER_SERVER} users : - name : zora-view user : token : ${TOKEN_VALUE} EOF","title":"4. Generate a kubeconfig file"},{"location":"target-cluster/#verify-the-generated-kubeconfig","text":"These steps create a file in your current working directory called zora-view-kubeconfig.yml . The contents of this file are used in the next guide to connect this target cluster into Zora. Before using this kubeconfig, you can verify that it is functional by running: kubectl --kubeconfig zora-view-kubeconfig.yml get all --all-namespaces","title":"Verify the generated kubeconfig"}]}